import re
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
import joblib
import pandas as pd
from datetime import datetime
import requests
import json
from config import Config

class AdvancedAIAnalyzer:
    def __init__(self):
        self.feature_names = [
            'title_ai_keyword_density', 'description_ai_keyword_density',
            'engagement_anomaly_score', 'upload_frequency_score',
            'title_sensationalism', 'description_complexity',
            'comment_sentiment_variance', 'metadata_consistency_score',
            'channel_ai_specialization', 'content_pattern_regularity'
        ]
        self.model = None
        self.scaler = StandardScaler()
        
    def extract_advanced_features(self, video_data, channel_history=None):
        """Extract sophisticated features for AI content detection"""
        features = []
        
        # 1. Textual Analysis Features
        features.extend(self._analyze_text_patterns(video_data))
        
        # 2. Behavioral Features
        features.extend(self._analyze_behavioral_patterns(video_data, channel_history))
        
        # 3. Temporal Features
        features.extend(self._analyze_temporal_patterns(video_data, channel_history))
        
        # 4. Network Features
        features.extend(self._analyze_network_patterns(video_data))
        
        return np.array(features)
    
    def _analyze_text_patterns(self, video_data):
        """Advanced text analysis beyond simple keywords"""
        title = video_data.get('title', '')
        description = video_data.get('description', '')
        
        # 1. AI keyword density with context
        ai_phrases = [
            'created with AI', 'generated by', 'AI tool', 'neural network',
            'machine learning model', 'synthetic media', 'deepfake',
            'GAN generated', 'diffusion model', 'prompt engineering'
        ]
        
        title_words = len(title.split())
        desc_words = len(description.split())
        
        title_score = sum(1 for phrase in ai_phrases if phrase.lower() in title.lower()) / max(1, title_words)
        desc_score = sum(1 for phrase in ai_phrases if phrase.lower() in description.lower()) / max(1, desc_words)
        
        # 2. Title sensationalism score
        sensational_words = ['SHOCKING', 'AMAZING', 'INCREDIBLE', 'MIND-BLOWING', 'UNBELIEVABLE', 'BREAKING']
        sensational_score = sum(1 for word in sensational_words if word in title.upper()) / max(1, title_words)
        
        # 3. Description complexity (AI content often has technical descriptions)
        desc_complexity = min(len(description.split()) / 100, 1.0)  # Normalized
        
        return [title_score, desc_score, sensational_score, desc_complexity]
    
    def _analyze_behavioral_patterns(self, video_data, channel_history):
        """Analyze upload and engagement patterns"""
        stats = video_data.get('stats', {})
        views = stats.get('viewCount', 1)
        likes = stats.get('likeCount', 0)
        comments = stats.get('commentCount', 0)
        
        # 1. Engagement anomaly detection
        expected_engagement = 0.05  # 5% engagement rate typically
        actual_engagement = (likes + comments) / views
        engagement_anomaly = min(abs(actual_engagement - expected_engagement) * 10, 1.0)
        
        # 2. Like/Comment ratio anomaly
        like_comment_ratio = likes / max(1, comments)
        # AI content often has abnormal like/comment ratios
        ratio_anomaly = min(abs(like_comment_ratio - 10) / 20, 1.0)  # Normalize
        
        # 3. View velocity (simplified)
        view_velocity = self._calculate_view_velocity(video_data, channel_history)
        
        return [engagement_anomaly, ratio_anomaly, view_velocity]
    
    def _analyze_temporal_patterns(self, video_data, channel_history):
        """Analyze timing and frequency patterns"""
        # 1. Upload frequency consistency
        upload_consistency = self._calculate_upload_consistency(channel_history)
        
        # 2. Content regularity
        pattern_regularity = self._calculate_pattern_regularity(channel_history)
        
        return [upload_consistency, pattern_regularity]
    
    def _analyze_network_patterns(self, video_data):
        """Analyze channel network and metadata patterns"""
        # 1. Channel specialization in AI content
        channel_title = video_data.get('channel_title', '').lower()
        ai_channel_indicators = ['ai', 'artificial', 'neural', 'machine learning', 'tech', 'future', 'digital art']
        channel_specialization = sum(1 for term in ai_channel_indicators if term in channel_title) / 3
        
        # 2. Metadata consistency
        metadata_consistency = self._check_metadata_consistency(video_data)
        
        return [min(channel_specialization, 1.0), metadata_consistency]
    
    def _calculate_view_velocity(self, video_data, channel_history):
        """Calculate how quickly views are accumulating"""
        # Simplified version - in reality you'd need historical data
        publish_time = video_data.get('published_at')
        if publish_time:
            try:
                # Very basic time-based estimation
                publish_date = datetime.fromisoformat(publish_time.replace('Z', '+00:00'))
                time_since_publish = (datetime.now().astimezone() - publish_date).total_seconds() / 3600  # hours
                
                views = video_data.get('stats', {}).get('viewCount', 0)
                if time_since_publish > 0:
                    views_per_hour = views / time_since_publish
                    # Normalize (1000 views/hour = max score)
                    return min(views_per_hour / 1000, 1.0)
            except:
                pass
        return 0.5
    
    def _calculate_upload_consistency(self, channel_history):
        """Calculate how consistent upload patterns are"""
        if not channel_history or len(channel_history) < 3:
            return 0.5
        
        # Simplified consistency calculation
        return 0.7
    
    def _calculate_pattern_regularity(self, channel_history):
        """Calculate regularity in content patterns"""
        if not channel_history:
            return 0.5
        return 0.6
    
    def _check_metadata_consistency(self, video_data):
        """Check consistency between title, description, and tags"""
        title = video_data.get('title', '').lower()
        description = video_data.get('description', '').lower()
        tags = video_data.get('tags', [])
        
        # Check if key terms appear consistently across metadata
        all_text = f"{title} {description} {' '.join(tags)}".lower()
        
        ai_terms = ['ai', 'artificial', 'generated', 'neural', 'machine learning']
        term_consistency = []
        
        for term in ai_terms:
            appearances = []
            appearances.append(1 if term in title else 0)
            appearances.append(1 if term in description else 0)
            appearances.append(1 if any(term in tag.lower() for tag in tags) else 0)
            
            # Only calculate if term appears at least once
            if sum(appearances) > 0:
                term_consistency.append(np.std(appearances))
        
        if term_consistency:
            return 1 - np.mean(term_consistency)  # Higher = more consistent
        return 0.5
    
    def train_model(self, training_data):
        """Train a simple ML model on extracted features"""
        X = np.array([self.extract_advanced_features(item['video_data']) for item in training_data])
        y = np.array([item['is_ai_content'] for item in training_data])
        
        # Scale features
        X_scaled = self.scaler.fit_transform(X)
        
        # Train lightweight Random Forest
        self.model = RandomForestClassifier(
            n_estimators=50,
            max_depth=10,
            random_state=42,
            n_jobs=1  # Use only one core for compatibility
        )
        self.model.fit(X_scaled, y)
        
        # Save model
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'trained_at': datetime.now().isoformat()
        }
        joblib.dump(model_data, Config.ML_MODEL_PATH)
        
        print(f"Model trained and saved to {Config.ML_MODEL_PATH}")
        return self.model
    
    def load_model(self):
        """Load pre-trained model"""
        try:
            loaded = joblib.load(Config.ML_MODEL_PATH)
            self.model = loaded['model']
            self.scaler = loaded['scaler']
            self.feature_names = loaded.get('feature_names', self.feature_names)
            print("Pre-trained model loaded successfully")
            return True
        except Exception as e:
            print(f"Could not load model: {e}")
            return False
    
    def predict(self, video_data, channel_history=None):
        """Predict if content is AI-generated"""
        if self.model is None and not self.load_model():
            # Fallback to rule-based scoring
            return self._fallback_prediction(video_data)
        
        try:
            features = self.extract_advanced_features(video_data, channel_history)
            features_scaled = self.scaler.transform(features.reshape(1, -1))
            probability = self.model.predict_proba(features_scaled)[0][1]
            return probability
        except Exception as e:
            print(f"Prediction error: {e}")
            return self._fallback_prediction(video_data)
    
    def _fallback_prediction(self, video_data):
        """Enhanced fallback to rule-based scoring"""
        score = 0
        
        # Text analysis
        title = video_data.get('title', '').lower()
        description = video_data.get('description', '').lower()
        
        # Advanced pattern matching with weights
        patterns = [
            (r'AI.*generated', 3), (r'created.*AI', 3), (r'neural.*network', 2),
            (r'machine.*learning', 2), (r'synthetic.*media', 3), (r'deepfake', 4),
            (r'GAN', 3), (r'diffusion.*model', 3), (r'prompt.*engineering', 3),
            (r'AI.*art', 2), (r'artificial.*intelligence', 2)
        ]
        
        for pattern, weight in patterns:
            if re.search(pattern, f"{title} {description}", re.IGNORECASE):
                score += weight
        
        # Engagement pattern analysis
        stats = video_data.get('stats', {})
        views = stats.get('viewCount', 0)
        likes = stats.get('likeCount', 0)
        comments = stats.get('commentCount', 0)
        
        if views > 0:
            engagement = (likes + comments) / views
            if engagement < 0.005:  # Very low engagement
                score += 2
            elif engagement > 0.2:  # Suspiciously high engagement
                score += 1
        
        # Channel specialization
        channel_title = video_data.get('channel_title', '').lower()
        if any(term in channel_title for term in ['ai', 'artificial', 'neural']):
            score += 2
        
        # Normalize to probability
        return min(score / 20, 1.0)